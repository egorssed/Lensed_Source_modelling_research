{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beta_VAE_Controlled_C.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "D5E0M8R4R1g1",
        "F7dCWzjiTKFY",
        "GwcB3clVT27r",
        "7cIM3MCbxKk9",
        "_G3IKQOVV8hW",
        "GTdpTcnQh17t"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/egorssed/Lensed_Source_modelling_research/blob/main/Beta_VAE_Controlled_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--yHcTSyM22Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea369b94-58d9-4334-a51f-0c05fdb82f9c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "Folder='/content/drive/My Drive/Lensed Source modelling research/'\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(Folder+'/Modules')\n",
        "import Image_Stats\n",
        "import Visual_analysis\n",
        "\n",
        "image_size=64\n",
        "batch_size = 32\n",
        "latent_dim = 64\n",
        "start_lr = 1e-6\n",
        "\n",
        "x = np.linspace(0, 64, 64)\n",
        "y = np.linspace(0, 64, 64)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "xdata = np.vstack((X.ravel(), Y.ravel()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5E0M8R4R1g1"
      },
      "source": [
        "### Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvyXq__bSCBU"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#We normalize the data to [0,1]\n",
        "#Initial minI and maxI are stored in the Dataset_labels\n",
        "#All the negative values set to 0 during preprocessing\n",
        "def normalize_image(images):\n",
        "    #Normalize image to [0,1]\n",
        "    for i in range(len(images)):\n",
        "        images[i][images[i]<0]=0\n",
        "        images[i]=images[i]/images[i].max()\n",
        "    return images\n",
        "\n",
        "#Read the images and the features from the google drive\n",
        "galaxy_images=np.load(Folder+'Data/Images_Filtered_Rotated.npy')\n",
        "df=pd.read_csv(Folder+'Data/Labels_Filtered.csv',index_col=0)\n",
        "\n",
        "#Bring images to a proper tensorial form\n",
        "Size_of_dataset=len(galaxy_images[:-(len(galaxy_images)%batch_size)])\n",
        "Number_of_batches=Size_of_dataset//batch_size\n",
        "df=df.iloc[:Size_of_dataset]\n",
        "galaxy_images=galaxy_images[:Size_of_dataset]\n",
        "\n",
        "#Bring images to a proper tensorial form\n",
        "gal_dataset_images=np.reshape(normalize_image(galaxy_images), (len(galaxy_images), galaxy_images.shape[1], galaxy_images.shape[2], 1))\n",
        "\n",
        "#Separate the features we are eager to predict in the latent space\n",
        "#Features=['Sersic_HLR', 'Sersic_n', 'Sersic_q','Sersic_phi']\n",
        "\n",
        "#Extract test dataset\n",
        "x_train, x_test ,y_train,y_test= train_test_split(gal_dataset_images.astype('float32'),df, test_size=round(0.05*Number_of_batches)*batch_size,random_state=42)\n",
        "#Extract train and validation datasets\n",
        "x_train, x_val , y_train, y_val= train_test_split(x_train,y_train, test_size=round(0.05*Number_of_batches)*batch_size,random_state=42)\n",
        "#All the sets have number of galaxies which is a multiple of the batch size (32)\n",
        "#The random_state is fixed so the split is reproducible yet random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7dCWzjiTKFY"
      },
      "source": [
        "### VAE "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM1cAPYUTPOU"
      },
      "source": [
        "#### libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5QEGFHlSal0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input, Dense, BatchNormalization,Flatten,Reshape,Lambda,Conv2D,Conv2DTranspose,LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras import initializers\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.framework import ops,smart_cond\n",
        "import keras.backend as K"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwcB3clVT27r"
      },
      "source": [
        "#### encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXWC6Kq1vXvO"
      },
      "source": [
        "def encoder_function(input_img):\n",
        "    #He initialization for Relu activated layers\n",
        "    Heinitializer = initializers.HeNormal()\n",
        "\n",
        "    x = Conv2D(filters=64, kernel_size=4, strides=2,padding='same',use_bias=False,kernel_initializer=Heinitializer)(input_img)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = Conv2D(filters=128, kernel_size=4, strides=2,padding='same',use_bias=False,kernel_initializer=Heinitializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = Conv2D(filters=256, kernel_size=4, strides=2,padding='same',use_bias=False,kernel_initializer=Heinitializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = Conv2D(filters=512, kernel_size=4, strides=2,padding='same',use_bias=False,kernel_initializer=Heinitializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = Conv2D(filters=4096, kernel_size=4, strides=1,padding='valid',use_bias=False,kernel_initializer=Heinitializer)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    \n",
        "    \n",
        "    #Predict mean of standard distribution and logarithm of variance\n",
        "    Xavierinitializer=initializers.GlorotNormal()\n",
        "\n",
        "    z_mean = Dense(latent_dim,kernel_initializer=Xavierinitializer,bias_initializer=Xavierinitializer)(x)\n",
        "    z_log_var = Dense(latent_dim, kernel_initializer=Xavierinitializer,bias_initializer=Xavierinitializer)(x)\n",
        "\n",
        "    return z_mean,z_log_var"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cIM3MCbxKk9"
      },
      "source": [
        "#### decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rHylOyNwk4J"
      },
      "source": [
        "def get_decoder(activation):\n",
        "  def decoder_function(z):\n",
        "    #He initialization for Relu activated layers\n",
        "    Heinitializer = initializers.HeNormal()\n",
        "\n",
        "    x = Reshape(target_shape=(1, 1, 64))(z)\n",
        "    \n",
        "    x = Conv2DTranspose(filters=512, kernel_size=4, strides=1,padding='valid',use_bias=False,kernel_initializer=Heinitializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(filters=256, kernel_size=4, strides=2,padding='same',use_bias=False,kernel_initializer=Heinitializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(filters=128, kernel_size=4, strides=2,padding='same',use_bias=False,kernel_initializer=Heinitializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    \n",
        "    x = Conv2DTranspose(filters=64, kernel_size=4, strides=2,padding='same',use_bias=False,kernel_initializer=Heinitializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "\n",
        "    #Xavier intialization for differentiable functions activated layers\n",
        "    Xavierinitializer=initializers.GlorotNormal()\n",
        "    if activation=='':\n",
        "      decoded = Conv2DTranspose(filters=1, kernel_size=4, strides=2,padding='same',use_bias=False\n",
        "                                ,kernel_initializer=Xavierinitializer)(x)\n",
        "    else:\n",
        "      decoded = Conv2DTranspose(filters=1, kernel_size=4, strides=2,padding='same',use_bias=False,\n",
        "                              activation=activation,kernel_initializer=Xavierinitializer)(x)\n",
        "  \n",
        "    return decoded\n",
        "  return decoder_function"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THhYPzXbB2Wh"
      },
      "source": [
        "#### loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtYDc_gREknl"
      },
      "source": [
        "Decorators meant to create loss functions with given hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vl6ue3PB-32"
      },
      "source": [
        "def get_reconstruction_loss(loss_type='chi_sq'):\n",
        "\n",
        "  #Chose original loss function\n",
        "  if (loss_type=='chi_sq') or (loss_type=='mse'):\n",
        "      loss_function=tf.math.squared_difference\n",
        "  if loss_type=='binary_crossentropy':\n",
        "      loss_function=K.binary_crossentropy\n",
        "\n",
        "  def reconstruction_loss_function(y_true,y_pred):\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\n",
        "\n",
        "    reconstruction_loss=loss_function(y_true,y_pred)\n",
        "\n",
        "    #cast mse to chi_square by adding weights\n",
        "    if (loss_type=='chi_sq'):\n",
        "      #Poisson weights (sigma=sqrt(image))\n",
        "      weights=tf.math.pow(tf.sqrt(tf.abs(y_true)+1e-5),-1)\n",
        "      weights= math_ops.cast(weights, y_pred.dtype)\n",
        "      reconstruction_loss=reconstruction_loss*weights\n",
        "    \n",
        "    return K.sum(reconstruction_loss,axis=-1)\n",
        "  \n",
        "  return reconstruction_loss_function\n",
        "\n",
        "def get_regularization_loss(beta_vae=1e-3,gamma=None):\n",
        "  #Capacity is a global variable that is changed during training\n",
        "  def regularization_loss_function(mean,logvar):\n",
        "    KL_loss=0.5 * K.sum(K.exp(logvar)+K.square(mean)-1-logvar, axis=-1)\n",
        "    if (gamma is not None) and (Capacity is not None):  \n",
        "      return gamma*tf.abs(KL_loss-Capacity)\n",
        "    else:\n",
        "      return beta_vae*KL_loss\n",
        "  \n",
        "  return regularization_loss_function"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9dJ3J1ZERYp"
      },
      "source": [
        "def get_model_loss(beta_vae=1e-3,loss_type='chi_sq',gamma=None):\n",
        "\n",
        "  reconstruction_loss_function=get_reconstruction_loss(loss_type)\n",
        "  regularization_loss_function=get_regularization_loss(beta_vae,gamma)\n",
        "\n",
        "  def model_loss_function(x,decoded):\n",
        "\n",
        "    #reconstruction quality\n",
        "    flattened_x=K.reshape(x,shape=(len(x),image_size*image_size))\n",
        "    flattened_decoded=K.reshape(decoded,shape=(len(decoded),image_size*image_size))\n",
        "    reconstruction_loss=reconstruction_loss_function(flattened_x,flattened_decoded)\n",
        "\n",
        "    #KL divergence regularization quality\n",
        "    mean = models['z_meaner'](x)\n",
        "    logvar=models['z_log_varer'](x)\n",
        "    regularization_loss=regularization_loss_function(mean,logvar)\n",
        "\n",
        "    Beta_VAE_Loss=(reconstruction_loss+regularization_loss)/image_size/image_size\n",
        "    return Beta_VAE_Loss\n",
        "\n",
        "  return model_loss_function,reconstruction_loss_function,regularization_loss_function"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G3IKQOVV8hW"
      },
      "source": [
        "#### vae class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-QxAMyevNZS"
      },
      "source": [
        "def create_vae(beta_vae=1e-3,activation='softplus',loss_type='chi_sq',gamma=None):\n",
        "\n",
        "    #Reparametrization trick\n",
        "    def reparameterize(args):\n",
        "      mean,logvar=args\n",
        "      eps = tf.random.normal(shape=mean.shape)\n",
        "      return eps * tf.exp(logvar/2) + mean\n",
        "    decoder_function=get_decoder(activation)\n",
        "    model_loss_function,reconstruction_loss_function,regularization_loss_function=get_model_loss(beta_vae,loss_type,gamma)\n",
        "\n",
        "\n",
        "    #VAE\n",
        "    models = {}\n",
        "\n",
        "    #Encoder\n",
        "    input_img = Input(batch_shape=(batch_size, image_size, image_size, 1))\n",
        "\n",
        "    z_mean, z_log_var=encoder_function(input_img)\n",
        "\n",
        "    l=Lambda(reparameterize, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "    models[\"encoder\"]  = Model(input_img, l, name='Encoder') \n",
        "    models[\"z_meaner\"] = Model(input_img, z_mean, name='Enc_z_mean')\n",
        "    models[\"z_log_varer\"] = Model(input_img, z_log_var, name='Enc_z_log_var')\n",
        "\n",
        "    #Decoder\n",
        "    z = Input(shape=(latent_dim, ))\n",
        "    decoded=decoder_function(z)\n",
        "\n",
        "    models[\"decoder\"] = Model(z, decoded, name='Decoder')\n",
        "    models[\"vae\"]     = Model(input_img, models[\"decoder\"](models[\"encoder\"](input_img)), name=\"VAE\")\n",
        "\n",
        "    return models,model_loss_function,reconstruction_loss_function,regularization_loss_function"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TLVkAykF_ou"
      },
      "source": [
        "def get_Neural_Network(beta_vae=1e-3,activation='softplus',loss_type='chi_sq',gamma=None):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    models,model_loss_function,reconstruction_loss_function,regularization_loss_function =create_vae(beta_vae,activation,loss_type,gamma)\n",
        "\n",
        "    models[\"vae\"].compile(optimizer=Adam(learning_rate=start_lr, beta_1=0.5, beta_2=0.999,clipvalue=1), loss=model_loss_function)\n",
        "\n",
        "    return models,model_loss_function,reconstruction_loss_function,regularization_loss_function"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3QJOtB2Q7o7"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2q122NDGdtc"
      },
      "source": [
        "beta_vae=None\n",
        "gamma=1e+3\n",
        "Start_Capacity=0\n",
        "Capacity_speed=1\n",
        "activation='softplus'\n",
        "loss_type='chi_sq'\n",
        "models,model_loss_function,reconstruction_loss_function,regularization_loss_function=get_Neural_Network(beta_vae,activation,loss_type,gamma)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfRq35aCQ_hz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14238ab-18e5-4770-973a-0f4d151c9acd"
      },
      "source": [
        "Checkpoint_to_load=0\n",
        "Checkpoint_name='Models/Softplus_chisq_Capacity_control/gamma=1e+3_Start_C=0_Speed_1_per_epoch_clipvalue=1/'\n",
        "Checkpoints_Folder=Folder+Checkpoint_name\n",
        "\n",
        "Capacity=Start_Capacity+(Checkpoint_to_load)*Capacity_speed\n",
        "\n",
        "if Checkpoint_to_load==0:\n",
        "  try:\n",
        "    os.mkdir(Checkpoints_Folder)\n",
        "  except OSError as error:\n",
        "    print(error)\n",
        "#models['vae'].load_weights(Checkpoints_Folder+'epoch_{}/Model'.format(Checkpoint_to_load))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 17] File exists: '/content/drive/My Drive/Lensed Source modelling research/Models/Softplus_chisq_Capacity_control/gamma=1e+3_Start_C=0_Speed_1_per_epoch_clipvalue=1/'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTdpTcnQh17t"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwByieNchver"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from keras.callbacks import LambdaCallback, ReduceLROnPlateau, TensorBoard\n",
        "import seaborn as sns\n",
        "\n",
        "# Отслеживать будем на вот этих цифрах\n",
        "indices_to_compare=np.zeros(32,dtype=int)\n",
        "indices_to_compare[:9]=[0,34,29,58,160,201,314,425,740]\n",
        "imgs = x_test[indices_to_compare]\n",
        "n_compare = 9\n",
        "\n",
        "def chisq_Poisson(y_true,y_pred):\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\n",
        "\n",
        "    reconstruction_loss=tf.math.squared_difference(y_true,y_pred)\n",
        "\n",
        "    weights=tf.math.pow(tf.sqrt(tf.abs(y_true)+1e-5),-1)\n",
        "    weights= math_ops.cast(weights, y_pred.dtype)\n",
        "    reconstruction_loss=reconstruction_loss*weights\n",
        "    \n",
        "    return K.eval(K.sum(reconstruction_loss,axis=-1))\n",
        "\n",
        "def DKL_per_variable(z_means,z_log_vars):\n",
        "    DKL=-0.5 * K.mean(1 + z_log_vars - K.square(z_means) - K.exp(z_log_vars), axis=0)\n",
        "    sns.barplot(x=np.linspace(0,64,64),y=np.sort(DKL)[::-1])\n",
        "    plt.xticks([])\n",
        "    plt.xlabel('Latent variable')\n",
        "    plt.ylabel('nats')\n",
        "    plt.title(r'KL_div')\n",
        "    plt.show()\n",
        "    return K.eval(DKL)\n",
        "\n",
        "def on_epoch_end(epoch=0, logs=''):\n",
        "  global Capacity\n",
        "  Epoch_Capacity=Start_Capacity+(epoch+Checkpoint_to_load)*Capacity_speed\n",
        "\n",
        "  #Assign new value to the global variable Capacity\n",
        "  Capacity=Epoch_Capacity\n",
        "  #Every 10 epoch do the following\n",
        "  if np.mod(epoch,10)==0:\n",
        "        clear_output()\n",
        "        \n",
        "        decoded = models['vae'].predict(x_test, batch_size=batch_size)\n",
        "        Visual_analysis.plot_galaxies(imgs[:n_compare], decoded[indices_to_compare[:n_compare]])\n",
        "        Visual_analysis.plot_galaxies(imgs[:n_compare], decoded[indices_to_compare[:n_compare]],dimensions='3d')\n",
        "\n",
        "        #Also take a look on how latent space variables are distributed\n",
        "        Visual_analysis.Show_latent_distr(models,x_test,1,reconstruction_loss_function)\n",
        "\n",
        "        mean = models['z_meaner'](x_test)\n",
        "        logvar=models['z_log_varer'](x_test)\n",
        "        regularization_loss=regularization_loss_function(mean,logvar)\n",
        "        plt.hist(regularization_loss,bins=40)\n",
        "        plt.title('Reg loss')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        DKL=DKL_per_variable(mean,logvar)\n",
        "        print('Controlled Capacity',Epoch_Capacity)\n",
        "        print('Real Capacity',DKL.sum())\n",
        "\n",
        "        flattened_x=K.reshape(x_test,shape=(len(x_test),image_size*image_size))\n",
        "        flattened_decoded=K.reshape(decoded,shape=(len(decoded),image_size*image_size))\n",
        "        fig,ax=plt.subplots(1,3,figsize=(10,5))\n",
        "        maxima=K.eval(flattened_decoded).max(axis=1)\n",
        "        maxima_RAE=np.abs(maxima-1)\n",
        "        ax[0].hist(maxima,bins=20)\n",
        "        ax[0].set_title('Distribution of reconstructed maximum')\n",
        "        chi_squares=chisq_Poisson(flattened_x,flattened_decoded)\n",
        "        ax[1].hist(chi_squares,bins=40)\n",
        "        ax[1].set_title(r'$\\chi^2$')\n",
        "        #ax[1].set_xscale('log')\n",
        "        SSIM=tf.image.ssim(x_test.astype('double'), decoded.astype('double'),max_val=1,filter_size=8).numpy()\n",
        "        ax[2].hist(SSIM,bins=20)\n",
        "        ax[2].set_title('SSIM')\n",
        "        plt.show()\n",
        "\n",
        "        print('Max values RAE {:.3f} ± {:.3f}, median {:.3f}'.format(maxima_RAE.mean(),maxima_RAE.std(),np.median(maxima_RAE)))\n",
        "        print(r'reduced chi^2 values {:.3f} ± {:.3f}, median {:.3f}'.format(chi_squares.mean(),chi_squares.std(),np.median(chi_squares)))\n",
        "        print('SSIM values {:.3f} ± {:.3f}, median {:.3f}'.format(SSIM.mean(),SSIM.std(),np.median(SSIM)))\n",
        "\n",
        "        #Make a checkpoint of model weights and logs on Google drive\n",
        "        if epoch!=0:\n",
        "          Model_folder=Checkpoints_Folder+'epoch_{}/Model'.format(Checkpoint_to_load+epoch)\n",
        "          models['vae'].save_weights(Model_folder)\n",
        "\n",
        "          #Write loss logs\n",
        "          file_object = open(Checkpoints_Folder+'logs.txt', 'a')\n",
        "          file_object.write(str(logs))\n",
        "          file_object.write('\\n')\n",
        "          file_object.close()\n",
        "\n",
        "          #Write KL divergence of latent space \n",
        "          file_object = open(Checkpoints_Folder+'DKL.txt', 'a')\n",
        "          file_object.write(str(DKL))\n",
        "          file_object.write('\\n')\n",
        "          file_object.close()\n",
        "\n",
        "          Visual_analysis.learning_curve(Checkpoints_Folder+'logs.txt',np.maximum(Checkpoint_to_load+epoch-200,0),Checkpoint_to_load+epoch)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "# Коллбэки\n",
        "pltfig = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "lr_red = ReduceLROnPlateau(factor=0.1, patience=10, min_lr=1e-10,verbose=1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgCsoj_iihj6"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIrjblWAis6n",
        "outputId": "cd752b35-ae77-47df-d01c-1c9581dde3d4"
      },
      "source": [
        "_ = models['vae'].fit(x_train, x_train, shuffle=True, epochs=900,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_val, x_val),\n",
        "        callbacks=[pltfig,lr_red],\n",
        "        verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/900\n",
            " 84/541 [===>..........................] - ETA: 1:24 - loss: 39.9581"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZENx0ggQNqYc"
      },
      "source": [
        ""
      ]
    }
  ]
}